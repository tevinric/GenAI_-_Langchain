{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bKQIsIq-d8y"
      },
      "source": [
        "#**Llama 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnV5UC7A2vBZ"
      },
      "source": [
        "The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC41zK5l3Abp"
      },
      "source": [
        " It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nobX9E83PjQ"
      },
      "source": [
        "[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K4QuEDH4CbY"
      },
      "source": [
        "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
        "\n",
        "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YC846SH5DOK"
      },
      "source": [
        "#  Quantized Models from the Hugging Face Community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TD82wis5LGA"
      },
      "source": [
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
        "\n",
        "\n",
        "\n",
        "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQZBmz7I5neU"
      },
      "source": [
        "#**Step 1: Install All the Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0avf7xx2lcj",
        "outputId": "6b84e57f-aaff-433e-ede6-f297051b34a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in c:\\users\\tevin\\anaconda3\\lib\\site-packages (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.62.3)\n",
            "Requirement already satisfied: requests in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.26.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.2)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Command errored out with exit status 1:\n",
            "   command: 'C:\\Users\\tevin\\anaconda3\\python.exe' 'C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\tevin\\AppData\\Local\\Temp\\tmp9z_h0hgj'\n",
            "       cwd: C:\\Users\\tevin\\AppData\\Local\\Temp\\pip-install-i4a8i2hi\\llama-cpp-python_7faa970e69b9497fac78f86ad0f3dbbd\n",
            "  Complete output (380 lines):\n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from llama-cpp-python==0.1.78) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from llama-cpp-python==0.1.78) (4.9.0)\n",
            "Collecting diskcache>=5.6.1\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (PEP 517): started\n",
            "  Building wheel for llama-cpp-python (PEP 517): finished with status 'error'\n",
            "Failed to build llama-cpp-python\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  -- The C compiler identification is unknown\n",
            "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
            "    No CMAKE_C_COMPILER could be found.\n",
            "  \n",
            "    Tell CMake where to find the compiler by setting either the environment\n",
            "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
            "    the compiler, or to the compiler name if it is in the PATH.\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Visual Studio 17 2022 x64 v143' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Generator\n",
            "  \n",
            "      Visual Studio 17 2022\n",
            "  \n",
            "    could not find any instance of Visual Studio.\n",
            "  \n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Visual Studio 17 2022 x64 v143' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  -- The C compiler identification is unknown\n",
            "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
            "    No CMAKE_C_COMPILER could be found.\n",
            "  \n",
            "    Tell CMake where to find the compiler by setting either the environment\n",
            "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
            "    the compiler, or to the compiler name if it is in the PATH.\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Visual Studio 16 2019 x64 v142' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Generator\n",
            "  \n",
            "      Visual Studio 16 2019\n",
            "  \n",
            "    could not find any instance of Visual Studio.\n",
            "  \n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Visual Studio 16 2019 x64 v142' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  -- The C compiler identification is unknown\n",
            "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
            "    No CMAKE_C_COMPILER could be found.\n",
            "  \n",
            "    Tell CMake where to find the compiler by setting either the environment\n",
            "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
            "    the compiler, or to the compiler name if it is in the PATH.\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Visual Studio 15 2017 x64 v141' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Generator\n",
            "  \n",
            "      Visual Studio 15 2017\n",
            "  \n",
            "    could not find any instance of Visual Studio.\n",
            "  \n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Visual Studio 15 2017 x64 v141' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Running\n",
            "  \n",
            "     'nmake' '-?'\n",
            "  \n",
            "    failed with:\n",
            "  \n",
            "     no such file or directory\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Running\n",
            "  \n",
            "     'nmake' '-?'\n",
            "  \n",
            "    failed with:\n",
            "  \n",
            "     no such file or directory\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "  \n",
            "  \n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  Not searching for unused variables given on the command line.\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "  \n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "  \n",
            "  \n",
            "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
            "    Running\n",
            "  \n",
            "     'nmake' '-?'\n",
            "  \n",
            "    failed with:\n",
            "  \n",
            "     no such file or directory\n",
            "  \n",
            "  \n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator - failure\n",
            "  --------------------------------------------------------------------------------\n",
            "  \n",
            "                  ********************************************************************************\n",
            "                  scikit-build could not get a working generator for your system. Aborting build.\n",
            "  \n",
            "                  Building windows wheels for Python 3.9 requires Microsoft Visual Studio 2022.\n",
            "  Get it with \"Visual Studio 2017\":\n",
            "  \n",
            "    https://visualstudio.microsoft.com/vs/\n",
            "  \n",
            "  Or with \"Visual Studio 2019\":\n",
            "  \n",
            "      https://visualstudio.microsoft.com/vs/\n",
            "  \n",
            "  Or with \"Visual Studio 2022\":\n",
            "  \n",
            "      https://visualstudio.microsoft.com/vs/\n",
            "  \n",
            "                  ********************************************************************************\n",
            "  ----------------------------------------\n",
            "  ERROR: Failed building wheel for llama-cpp-python\n",
            "ERROR: Could not build wheels for llama-cpp-python which use PEP 517 and cannot be installed directly\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.23.4\n",
            "  Downloading numpy-1.23.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "Successfully installed numpy-1.23.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
            "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.4 which is incompatible.\n",
            "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.23.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.37.tar.gz (10.8 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Command errored out with exit status 1:\n",
            "   command: 'c:\\Users\\tevin\\anaconda3\\python.exe' 'c:\\Users\\tevin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\tevin\\AppData\\Local\\Temp\\tmpty0ghict'\n",
            "       cwd: C:\\Users\\tevin\\AppData\\Local\\Temp\\pip-install-8sw8sh8f\\llama-cpp-python_68332ebbf54e41cca54b34bcdc223e90\n",
            "  Complete output (20 lines):\n",
            "  *** scikit-build-core 0.8.0 using CMake 3.28.1 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  2024-01-31 17:09:52,843 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
            "  loading initial cache file C:\\Users\\tevin\\AppData\\Local\\Temp\\tmpa269dbj9\\build\\CMakeInit.txt\n",
            "  -- Building for: NMake Makefiles"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting diskcache>=5.6.1\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\tevin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.9.0)\n",
            "Collecting jinja2>=2.11.3\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.4-cp39-cp39-win_amd64.whl (17 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (PEP 517): started\n",
            "  Building wheel for llama-cpp-python (PEP 517): finished with status 'error'\n",
            "Failed to build llama-cpp-python\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  CMake Error at CMakeLists.txt:3 (project):\n",
            "    Running\n",
            "  \n",
            "     'nmake' '-?'\n",
            "  \n",
            "    failed with:\n",
            "  \n",
            "     no such file or directory\n",
            "  \n",
            "  \n",
            "  CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
            "  CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  \n",
            "  *** CMake configuration failed\n",
            "  ----------------------------------------\n",
            "  ERROR: Failed building wheel for llama-cpp-python\n",
            "ERROR: Could not build wheels for llama-cpp-python which use PEP 517 and cannot be installed directly\n"
          ]
        }
      ],
      "source": [
        "pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qJ90LnMv54Y-"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lOmpKB36RJh"
      },
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ak3ZtGjM6Wdp"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "85XOzmui6rGN"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_cpp'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6988/1009111461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haAb9kNm6J9n"
      },
      "source": [
        "#**Step 3: Download the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "10524e48f4b547718f9892a97b74aedf",
            "92212bf063814297b2cab5b5fd7debb4",
            "770db95a57f743f0a3e792f7958892ad",
            "a7700b22453c413893e67246ce7a46d4",
            "588a5efd92c14a889a155f09e9e5172c",
            "8719b31162724024b14beedcb6c36dd0",
            "91db638aecc54d8691406925dedd5ca3",
            "6c72e588e98f4b4eb16d1b00861747e6",
            "9e81ef593c3e40289d830e07f3812e7e",
            "6c5b443689d74809aacec6cdf5d06b51",
            "71a380d81c1e4631bbe90c81f485e610"
          ]
        },
        "id": "qBgdGV4b6MxG",
        "outputId": "7e79c1c2-4305-46d8-ae46-ba6bc86dc6c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10524e48f4b547718f9892a97b74aedf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ6OYnI46kKq"
      },
      "source": [
        "#**Step 4: Loading the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irftToUj6aWt",
        "outputId": "17581285-8cda-4e10-db1f-af6b51f1365d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Llama' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6988/1744225319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlcpp_llm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m lcpp_llm = Llama(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mn_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# CPU cores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Llama' is not defined"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG4Pylz662At",
        "outputId": "a8915ee7-2171-404f-fe8f-7184547d8f77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-M307R6_pT"
      },
      "source": [
        "#**Step 5: Create a Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RfzwELMC7Dyg"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a linear regression in python\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT8pg6zt7QzA"
      },
      "source": [
        "#**Step 6: Generating the Response**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0aF0qWUJ7OPK"
      },
      "outputs": [],
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJ1JgR68DDO",
        "outputId": "5a781d33-fb83-43af-be2d-3ac8856d7280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'cmpl-d8aa8ca3-7dc4-4b35-ad2d-1280f3a922bf', 'object': 'text_completion', 'created': 1694099155, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/a17885f653039bd07ed0f8ff4ecc373abf5425fd/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': \"SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression in python\\n\\nASSISTANT:\\n\\nTo write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\\n```\\nfrom sklearn.linear_model import LinearRegression\\nimport pandas as pd\\n\\n# Load your dataset into a Pandas DataFrame\\ndf = pd.read_csv('your_data.csv')\\n\\n# Create a linear regression object and fit the data\\nreg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\\n\\n# Print the coefficients\\nprint(reg.coef_)\\n```\\nThis will output the coefficients of the linear regression, which you can use to make predictions on new data. For example:\\n```\\n# Predict the value of y for a given set of x values\\nx_values = [1, 2, 3]\\ny_pred = reg.predict(pd.Series([x_values]))\\nprint(y_pred)\\n```\\nThis will output the predicted values of y for the given set of x values.\\n\\nPlease note that this is just an example and you may need to modify it to fit your specific needs, such as changing the input features or\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 256, 'total_tokens': 295}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qona58gX8oAn",
        "outputId": "64d45fb1-9790-4af7-88bf-d3ed0757c44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Write a linear regression in python\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "To write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\n",
            "```\n",
            "from sklearn.linear_model import LinearRegression\n",
            "import pandas as pd\n",
            "\n",
            "# Load your dataset into a Pandas DataFrame\n",
            "df = pd.read_csv('your_data.csv')\n",
            "\n",
            "# Create a linear regression object and fit the data\n",
            "reg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\n",
            "\n",
            "# Print the coefficients\n",
            "print(reg.coef_)\n",
            "```\n",
            "This will output the coefficients of the linear regression, which you can use to make predictions on new data. For example:\n",
            "```\n",
            "# Predict the value of y for a given set of x values\n",
            "x_values = [1, 2, 3]\n",
            "y_pred = reg.predict(pd.Series([x_values]))\n",
            "print(y_pred)\n",
            "```\n",
            "This will output the predicted values of y for the given set of x values.\n",
            "\n",
            "Please note that this is just an example and you may need to modify it to fit your specific needs, such as changing the input features or\n"
          ]
        }
      ],
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4uMV0zF8pQt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10524e48f4b547718f9892a97b74aedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92212bf063814297b2cab5b5fd7debb4",
              "IPY_MODEL_770db95a57f743f0a3e792f7958892ad",
              "IPY_MODEL_a7700b22453c413893e67246ce7a46d4"
            ],
            "layout": "IPY_MODEL_588a5efd92c14a889a155f09e9e5172c"
          }
        },
        "588a5efd92c14a889a155f09e9e5172c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5b443689d74809aacec6cdf5d06b51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c72e588e98f4b4eb16d1b00861747e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a380d81c1e4631bbe90c81f485e610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "770db95a57f743f0a3e792f7958892ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c72e588e98f4b4eb16d1b00861747e6",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e81ef593c3e40289d830e07f3812e7e",
            "value": 9763701888
          }
        },
        "8719b31162724024b14beedcb6c36dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91db638aecc54d8691406925dedd5ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92212bf063814297b2cab5b5fd7debb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8719b31162724024b14beedcb6c36dd0",
            "placeholder": "​",
            "style": "IPY_MODEL_91db638aecc54d8691406925dedd5ca3",
            "value": "Downloading (…)chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "9e81ef593c3e40289d830e07f3812e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7700b22453c413893e67246ce7a46d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5b443689d74809aacec6cdf5d06b51",
            "placeholder": "​",
            "style": "IPY_MODEL_71a380d81c1e4631bbe90c81f485e610",
            "value": " 9.76G/9.76G [01:09&lt;00:00, 238MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
